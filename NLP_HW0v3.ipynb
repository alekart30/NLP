{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP_HW0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4pvaeDpH4MEc"
      },
      "source": [
        "# Advanced NLP HW0\n",
        "\n",
        "Before starting the task please read thoroughly these chapters of Speech and Language Processing by Daniel Jurafsky & James H. Martin:\n",
        "\n",
        "•\tN-gram language models: https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
        "\n",
        "•\tNeural language models: https://web.stanford.edu/~jurafsky/slp3/7.pdf \n",
        "\n",
        "In this task you will be asked to implement the models described there.\n",
        "\n",
        "Build a text generator based on n-gram language model and neural language model.\n",
        "1.\tFind a corpus (e.g. http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt ), but you are free to use anything else of your interest\n",
        "2.\tPreprocess it if necessary (we suggest using nltk for that)\n",
        "3.\tBuild an n-gram model\n",
        "4.\tTry out different values of n, calculate perplexity on a held-out set\n",
        "5.\tBuild a simple neural network model for text generation (start from a feed-forward net for example). We suggest using tensorflow + keras for this task\n",
        "\n",
        "Criteria:\n",
        "1.\tData is split into train / validation / test, motivation for the split method is given\n",
        "2.\tN-gram model is implemented\n",
        "a.\tUnknown words are handled\n",
        "b.\tAdd-k Smoothing is implemented\n",
        "3.\tNeural network for text generation is implemented\n",
        "4.\tPerplexity is calculated for both models\n",
        "5.\tExamples of texts generated with different models are present and compared\n",
        "6.\tOptional: Try both character-based and word-based approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObQupt1QO0IT",
        "colab_type": "text"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-K3RkIqbMMe",
        "colab_type": "text"
      },
      "source": [
        "We need nltk 3.4.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7yubCcBZlJ7",
        "colab_type": "code",
        "outputId": "682433df-69ea-4a4d-a937-6dd5a72a3fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "! pip uninstall nltk"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling nltk-3.2.5:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/nltk-3.2.5.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/nltk/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled nltk-3.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aG3pTnNakgc",
        "colab_type": "code",
        "outputId": "69bc1028-a551-429a-940f-58c45ce2b605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "! pip install nltk==3.4.5"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\r\u001b[K     |▎                               | 10kB 25.1MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 276kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 327kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 337kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 368kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 378kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 389kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 399kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 419kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 430kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 440kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 460kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 471kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 624kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 645kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 655kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 665kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 675kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 686kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 696kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 706kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 716kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 737kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 747kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 757kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 768kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 778kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 788kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 798kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 808kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 829kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 839kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 849kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 860kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 870kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 880kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 890kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 901kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 921kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 931kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 942kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 952kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 962kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 972kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 983kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 993kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.0MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.0MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.0MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5) (1.12.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449908 sha256=90cafdca199db984873214b2ce676ff1b0446e319502bc06931274c1eb48ed67\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "Successfully installed nltk-3.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJT5gEEYarTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9GDvVAwas-K",
        "colab_type": "code",
        "outputId": "738342ad-a546-48c5-cb91-048bfad22723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "nltk.__version__"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.4.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfzgHRjlO1ie",
        "colab_type": "code",
        "outputId": "2705cc6f-9c42-4f2a-de8f-9db4578c7832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import inflect\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm import NgramCounter\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkBo4SVuCURS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\") # Let's make sure GPU is available!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aE6Vd09OfQl",
        "colab_type": "text"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QWnXjwINerb",
        "colab_type": "code",
        "outputId": "ce9bb944-63c1-4097-eb95-bee64b479b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "! wget -c https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-01 11:32:46--  https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4573338 (4.4M) [text/plain]\n",
            "Saving to: ‘shakespeare_input.txt’\n",
            "\n",
            "shakespeare_input.t 100%[===================>]   4.36M  3.93MB/s    in 1.1s    \n",
            "\n",
            "2020-03-01 11:32:48 (3.93 MB/s) - ‘shakespeare_input.txt’ saved [4573338/4573338]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzEaADmVOxRL",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeTtu3DL5bUF",
        "colab_type": "text"
      },
      "source": [
        "##Loading text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXbdy-ihRtB5",
        "colab_type": "text"
      },
      "source": [
        "Read text as one string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqVRFv2UOyyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_file = open(\"shakespeare_input.txt\", \"r\")\n",
        "input_string = input_file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TtvjR5vTnQg",
        "colab_type": "text"
      },
      "source": [
        "Number of characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nXQahbbRSkD",
        "colab_type": "code",
        "outputId": "ef242447-9ed5-40a7-a975-8f18e1989042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(input_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4573338"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puGnXkxh8W1y",
        "colab_type": "code",
        "outputId": "c063ff97-7c0e-4aaa-bde3-646060663f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "input_string[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlJIoJsHXf-h",
        "colab_type": "text"
      },
      "source": [
        "Perpesent text as list of sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMSWCWdXWHAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "text = sent_detector.tokenize(input_string.strip())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97w4UzOUmHNn",
        "colab_type": "text"
      },
      "source": [
        "Make some replacement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gz7buptmLfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replacement_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\",\n",
        "                        \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                        \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
        "                        \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                        \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                        \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                        \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"I will\",\n",
        "                        \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"i'd\": \"i would\",\n",
        "                        \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\", \n",
        "                        \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                        \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
        "                        \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
        "                        \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
        "                        \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
        "                        \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
        "                        \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "                        \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
        "                        \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "                        \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
        "                        \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n",
        "                        \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
        "                        \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
        "                        \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                        \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
        "                        \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
        "                        \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
        "                        \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
        "                        \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\",\n",
        "                        \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
        "                        \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
        "                        \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n",
        "                        \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
        "                        \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                        \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                        \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
        "                        \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n",
        "                        \"'t\": \" it\", \"'em\": \"them\", \"o'\": \"of\", \"'ll\": \" will\", \"ne'er\":\"never\", \"'ld\": \" would\", \"i'\": \"in\",\n",
        "                        \"'d\": \"ed\", \"'en \": \"ken \", \"'bout\":\"about\", \"'gainst\":\"against\", \"'scape\":\"escape\", \"'mongst\": \"amongst\", \n",
        "                        \"'n\": \"en\", \"e'er\":\"ever\", \"itwas\":\"it was\" }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUQ0HCLVQbdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace_phrases(sentence, phrases_dict):\n",
        "  for key, value in phrases_dict.items():\n",
        "    sentence = sentence.replace(key, value)\n",
        "  return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-cmc3rl7hmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replacer = partial(replace_phrases, phrases_dict=replacement_dict)\n",
        "text = list(map(replacer, text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E0ukgT3Xkah",
        "colab_type": "text"
      },
      "source": [
        "Tokenize each sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1ZBBYW5WUTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_in_words = list(map(nltk.word_tokenize, text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHR1lazDT0OY",
        "colab_type": "text"
      },
      "source": [
        "We need to make normalization:\n",
        "\n",
        "*remove non ascii characters from words\n",
        "\n",
        "*to lower_case???\n",
        "\n",
        "*replace numbers\n",
        "\n",
        "*remove stopwords???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK1JqiQU8MS0",
        "colab_type": "text"
      },
      "source": [
        "##Basic preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dROX7Nf1Ty4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1Mej41qWBuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9egm9mvTWDe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s\\<\\>\\/]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azQJn09RWeol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace_numbers(words):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06stlrdMXIwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3MCt8LvYCQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stem_words(words):\n",
        "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word)\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc8JNECV8n1E",
        "colab_type": "text"
      },
      "source": [
        "## Using only some preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtZ3fB9ZX396",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = replace_numbers(words)\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EO6_zlU9F_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_in_words_normalized = list(map(normalize, text_in_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flyz5fpg9kY5",
        "colab_type": "text"
      },
      "source": [
        "Let's pad sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQz-lbj6X0bg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def padder(sentence):\n",
        "  return list(nltk.lm.preprocessing.pad_both_ends(sentence,n=2))\n",
        "text_in_words_padded = list(map(padder, text_in_words_normalized))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCVeFRYn9p7i",
        "colab_type": "code",
        "outputId": "4e2f9c9e-7f8d-4a4f-a5b5-9d0d85512d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "text_in_words_padded[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " 'first',\n",
              " 'citizen',\n",
              " ':',\n",
              " 'before',\n",
              " 'we',\n",
              " 'proceed',\n",
              " 'any',\n",
              " 'further',\n",
              " ',',\n",
              " 'hear',\n",
              " 'me',\n",
              " 'speak',\n",
              " '.',\n",
              " '</s>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G69E_AlNmjzQ",
        "colab_type": "text"
      },
      "source": [
        "## Incapsulate into classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikGorQd6p6S7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextLoader():\n",
        "  \"\"\"\n",
        "  This class loads text from text file and tokenizes it by sentences\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def load_text_by_sentences(self, filename):\n",
        "    # read raw text\n",
        "    with open(filename, \"r\") as input_file: \n",
        "      input_string = input_file.read()\n",
        "    # split by sentences\n",
        "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    return sent_detector.tokenize(input_string.strip())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP8xCt9W97Lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Preprocessor():\n",
        "  \"\"\"\n",
        "  This class preprocesses text (splitted by sentences): replaces some phrases, removes non-ascii characters and etc.\n",
        "  \"\"\"\n",
        "  def __init__(self, n):\n",
        "    self.__n = n\n",
        "    self.__replacement_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\",\n",
        "                        \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                        \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
        "                        \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                        \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                        \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                        \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"I will\",\n",
        "                        \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"i'd\": \"i would\",\n",
        "                        \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\", \n",
        "                        \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                        \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
        "                        \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
        "                        \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
        "                        \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
        "                        \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
        "                        \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "                        \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
        "                        \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "                        \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
        "                        \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n",
        "                        \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
        "                        \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
        "                        \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                        \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
        "                        \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
        "                        \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
        "                        \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
        "                        \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\",\n",
        "                        \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
        "                        \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
        "                        \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n",
        "                        \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
        "                        \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                        \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                        \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
        "                        \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n",
        "                        \"'t\": \" it\", \"'em\": \"them\", \"o'\": \"of\", \"'ll\": \" will\", \"ne'er\":\"never\", \"'ld\": \" would\", \"i'\": \"in\",\n",
        "                        \"'d\": \"ed\", \"'en \": \"ken \", \"'bout\":\"about\", \"'gainst\":\"against\", \"'scape\":\"escape\", \"'mongst\": \"amongst\", \n",
        "                        \"'n\": \"en\", \"e'er\":\"ever\", \"itwas\":\"it was\" }\n",
        "\n",
        "  @staticmethod\n",
        "  def __replace_phrases(sentence, phrases_dict):\n",
        "    # replace sentences accordingly to given dictionary\n",
        "    for key, value in phrases_dict.items():\n",
        "      sentence = sentence.replace(key, value)\n",
        "    return sentence\n",
        "\n",
        "  @staticmethod\n",
        "  def __preprocess_sentence(words):\n",
        "    \"\"\"\n",
        "    Removes non-ascii characters, converts to lowercase, replaces numbers\n",
        "    \"\"\"\n",
        "    def remove_non_ascii(words):\n",
        "      \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "      new_words = []\n",
        "      for word in words:\n",
        "          new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "          new_words.append(new_word)\n",
        "      return new_words\n",
        "\n",
        "    def to_lowercase(words):\n",
        "      \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "      new_words = []\n",
        "      for word in words:\n",
        "          new_word = word.lower()\n",
        "          new_words.append(new_word)\n",
        "      return new_words\n",
        "      \n",
        "    def replace_numbers(words):\n",
        "      \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "      p = inflect.engine()\n",
        "      new_words = []\n",
        "      for word in words:\n",
        "          if word.isdigit():\n",
        "              new_word = p.number_to_words(word)\n",
        "              new_words.append(new_word)\n",
        "          else:\n",
        "              new_words.append(word)\n",
        "      return new_words\n",
        "\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = replace_numbers(words)\n",
        "    return words\n",
        "\n",
        "  @staticmethod\n",
        "  def __padder(sentence, n):\n",
        "      return list(nltk.lm.preprocessing.pad_both_ends(sentence,n))\n",
        "\n",
        "\n",
        "  def normalize(self, text_in_sentences):\n",
        "    # replace sentences according to given dictionary\n",
        "    replacer = partial(Preprocessor.__replace_phrases, phrases_dict=self.__replacement_dict)\n",
        "    text = list(map(replacer, text_in_sentences))\n",
        "\n",
        "    # tokenize each sentence into words\n",
        "    text_in_sentences_splitted_by_words = list(map(nltk.word_tokenize, text))\n",
        "    \n",
        "    # preprocess words in each sentences\n",
        "    text_in_sentences_splitted_by_words_normalized = list(map(Preprocessor.__preprocess_sentence,\\\n",
        "                                                              text_in_sentences_splitted_by_words))\n",
        "    # pad each sentence from begging and end by special characters <s> </s>\n",
        "    padder = partial(Preprocessor.__padder, n=self.__n)\n",
        "    text_prepared = list(map(padder, text_in_sentences_splitted_by_words_normalized))\n",
        "\n",
        "    return text_prepared\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvTM384L-A3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_loader = TextLoader()\n",
        "full_text = text_loader.load_text_by_sentences('shakespeare_input.txt')\n",
        "preprocessor = Preprocessor(n=5)\n",
        "full_text_preprocessed = preprocessor.normalize(full_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HMVYf8wArsO",
        "colab_type": "code",
        "outputId": "52a4a7e8-ef13-4185-f252-a1a509ab1e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(full_text_preprocessed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52482"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHtYfeAbAq08",
        "colab_type": "code",
        "outputId": "37f68962-6f46-4a0a-f55e-132181ad318a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(text_in_words_padded)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52482"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m7FEwRuO6og0"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dmq1GrdFS05",
        "colab_type": "text"
      },
      "source": [
        "Short pipeline description:\n",
        "1.   Load text in sentences\n",
        "2.   Split into train/val/test (in sentences)\n",
        "3.   Inside update and perplexity evaluation incoming sequence of sentences will be preprocessed: normalized and padded\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7HWfry45gdt",
        "colab_type": "text"
      },
      "source": [
        "##N gram model with k-add smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xWBs4AoC4JfO",
        "colab": {}
      },
      "source": [
        "class NgramModel:\n",
        "  def __init__(self, n, k, vocab = None):\n",
        "    \"\"\"Language model constructor\n",
        "    n -- n-gram size\n",
        "    vocab -- optional fixed vocabulary for the model\n",
        "    \"\"\"\n",
        "    self.n = n\n",
        "    self.vocab = vocab\n",
        "    self.k = k\n",
        "    # create instance of preprocessor to normalize incoming text\n",
        "    self.preprocessor = Preprocessor(n)\n",
        "\n",
        "  def prob(self, word, context=None):\n",
        "    \"\"\"This method returns probability of a word with given context: P(w_t | w_{t - 1}...w_{t - n + 1})\n",
        "    \n",
        "    For example:\n",
        "    >>> lm.prob('hello', context=('world',))\n",
        "    0.99988\n",
        "    \"\"\"\n",
        "    return (self.text_ngrams[tuple(context)+(word,)] + self.k) / (self.text_n_minus_one_grams[tuple(context)] + self.k*len(self.vocab))\n",
        "    \n",
        "\n",
        "  def generate_text(self, text_length):\n",
        "    \"\"\"This method generates random text of length \n",
        "    \n",
        "    For example\n",
        "    >>> lm.generate_text(2)\n",
        "    hello world\n",
        "\n",
        "    \"\"\"\n",
        "    # we begin from <s><s>... n-1 times\n",
        "    text = ['<s>'] * (self.n-1)\n",
        "    for j in range(text_length):\n",
        "      # evaluate probabilities of each word for current context\n",
        "      probs = np.zeros(shape=(len(self.vocab)))\n",
        "      for i, word in enumerate(self.vocab):\n",
        "        probs[i] = self.prob(word=word, context=text[j:])\n",
        "      # normalize probabilitities due to some computational issues\n",
        "      probs = np.asarray(probs).astype('float64')\n",
        "      probs = probs / np.sum(probs)\n",
        "      # generate word index accordingly to distribution\n",
        "      rv = np.random.multinomial(1, probs, 1)\n",
        "      idx = rv.argmax()\n",
        "      # add word to text\n",
        "      text.append(self.vocab[idx])\n",
        "    # postprocess generated text\n",
        "    str_text = ' '. join(text)\n",
        "    str_text = str_text.replace('<s> ', \"\")\n",
        "    str_text = str_text.replace(' </s>', \"\")\n",
        "    return str_text\n",
        "\n",
        "  def update(self, sequence_of_tokens):\n",
        "    \"\"\"This method learns probabiities based on given sequence of tokents\n",
        "    \n",
        "    sequence_of_tokens -- iterable of tokens\n",
        "\n",
        "    For example\n",
        "    >>> lm.update(['hello', 'world'])\n",
        "    \"\"\"\n",
        "    # preprocess input sequence\n",
        "    normalized_sequence = self.preprocessor.normalize(sequence_of_tokens)\n",
        "\n",
        "    # merge into one list of words\n",
        "    words_list = []\n",
        "    for sent in normalized_sequence:\n",
        "      words_list += sent \n",
        "    # calculate ngrams and n-1 grams  \n",
        "    self.text_ngrams = Counter(ngrams(words_list, self.n))\n",
        "    self.text_n_minus_one_grams = Counter(ngrams(words_list, self.n-1))\n",
        "\n",
        "    # save vocabulary\n",
        "    self.vocab = list(set(words_list))\n",
        "    \n",
        "  def perplexity(self, sequence_of_tokens):\n",
        "    \"\"\"This method returns perplexity for a given sequence of tokens\n",
        "    \n",
        "    sequence_of_tokens -- iterable of tokens\n",
        "    \"\"\"\n",
        "    # preprocess input sequence\n",
        "    normalized_sequence = self.preprocessor.normalize(sequence_of_tokens)\n",
        "\n",
        "    # merge into one list of words\n",
        "    words_list = []\n",
        "    for sent in normalized_sequence:\n",
        "      words_list += sent \n",
        "\n",
        "    log_prob = 0\n",
        "    for i in range(self.n-1,len(words_list)):\n",
        "      log_prob += np.log(self.prob(word=words_list[i],\n",
        "                                   context=words_list[i-(self.n-1):i]))\n",
        "    return np.exp(-log_prob/len(words_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FURKDw6l5sTl",
        "colab_type": "text"
      },
      "source": [
        "Load text and split it into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utm07tZvECIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split(full_text, train_fraction, val_fraction):\n",
        "  length = len(full_text)\n",
        "  train, val, test = full_text[:int(train_fraction*length)],\\\n",
        "                     full_text[int(train_fraction*length):int((train_fraction+val_fraction)*length)],\\\n",
        "                     full_text[int((train_fraction+val_fraction)*length):]\n",
        "  return train, val, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH-LEbk55w2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_loader = TextLoader()\n",
        "full_text = text_loader.load_text_by_sentences(\"shakespeare_input.txt\")\n",
        "train_text, val_text, test_text = split(full_text, 0.5, 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKFAJUTFYpJy",
        "colab_type": "code",
        "outputId": "7a9d7670-7b07-4a35-f485-5b1ec1a132b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "model = NgramModel(n=5,k=1)\n",
        "model.update(train_text)\n",
        "print(model.perplexity(val_text))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2652.3228435544115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J21vit1y7R-b",
        "colab_type": "text"
      },
      "source": [
        "Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF1pUoYf7iJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perplexity_list = []\n",
        "for n in range(2,7):\n",
        "  model = NgramModel(n=n,k=1)\n",
        "  model.update(train_text)\n",
        "  perplexity_list.append(model.perplexity(val_text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow85OUYx86nJ",
        "colab_type": "code",
        "outputId": "df17dd30-fe00-4450-85c6-e4a15ed9d64d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plt.plot(list(range(2,7)), perplexity_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fdd51f778d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0TfGmCDy/wHZfJpr717ArAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbN5jDMQZgzd",
        "colab_type": "text"
      },
      "source": [
        "We choose model with minimal perplexity, which implies maximum probability of validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn_lAW3JZqjW",
        "colab_type": "code",
        "outputId": "6367543e-6e10-42d2-ab47-0c3d3fe4e8bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "model = NgramModel(n=2,k=1)\n",
        "model.update(train_text)\n",
        "print(model.perplexity(test_text))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1117.6749768007317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30nj_OH1j8rf",
        "colab_type": "code",
        "outputId": "0b1ff537-83b6-405a-a1b1-3c641e32aa64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(model.generate_text(100))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "truly scold ought clearest unconsidered nourished all-ending mouth-made swear fist lute-string superstitiously wallets adultress repossess judgment-day ross hates necessities barren discovery poll contract assuredly astonish spoils roguish cradles banners abode spectatorship marble-constant fearful'st faced john lovingly pastime divers changes well-divided 'true surmise foxes big-swoln defend reeking revolted jealousies unloads browsed'st camp gets 'kind achieves hostage helpers christophero placed wanderers temple-garden twelvemonth cneius unreversed sympathy look shear sought'st presumption speken flinty oped humility magnifiest stony-stratford bribes huge oferween lamented accusing tithing bewail curiously tamora bunting charm didofs gambols leagues rich-jeweled rents pursed woolvish branches forsooth rocky-hard unseparable blaze nursing cook plainly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ASackgH7AJ3",
        "colab_type": "text"
      },
      "source": [
        "## Neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7ZZJCkW7EWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralModel:\n",
        "  def __init__(self, n, n_epochs, vocab = None):\n",
        "    \"\"\"Language model constructor\n",
        "    n -- n-gram size\n",
        "    vocab -- optional fixed vocabulary for the model\n",
        "    \"\"\"\n",
        "    self.n = n\n",
        "    self.vocab = vocab\n",
        "    self.n_epochs = n_epochs\n",
        "    # create instance of preprocessor to normalize incoming text\n",
        "    self.preprocessor = Preprocessor(n)\n",
        "\n",
        "\n",
        "  def log_prob(self, word, context=None):\n",
        "    \"\"\"This method returns probability of a word with given context: P(w_t | w_{t - 1}...w_{t - n + 1})\n",
        "    \n",
        "    For example:\n",
        "    >>> lm.prob('hello', context=('world',))\n",
        "    0.99988\n",
        "    \"\"\"\n",
        "    # enter eval mode\n",
        "    self.nn_model.eval()\n",
        "    # transform context in torch tensor\n",
        "    context = np.array([self.word_to_id.get(word, self.word_to_id['unk']) for word in context])\n",
        "    context_idxs = torch.tensor(context.reshape(1,-1), dtype=torch.long)\n",
        "    # sent to gpu\n",
        "    x_gpu = context_idxs.to(device)   \n",
        "    # make prediction\n",
        "    log_probs = self.nn_model(x_gpu).cpu().detach().numpy()\n",
        "    # return lop_prob for our word\n",
        "    return log_probs[0,self.word_to_id.get(word, self.word_to_id['unk'])]\n",
        "    \n",
        "  def predict_distribution(self, context):\n",
        "    \"\"\"This method predicts whole words distribution for given context\"\"\"\n",
        "    # enter eval mode\n",
        "    self.nn_model.eval()\n",
        "    # transform context in torch tensor\n",
        "    context = np.array([self.word_to_id.get(word, self.word_to_id['unk']) for word in context])\n",
        "    context_idxs = torch.tensor(context.reshape(1,-1), dtype=torch.long)\n",
        "    # sent to gpu\n",
        "    x_gpu = context_idxs.to(device)\n",
        "    # make prediction\n",
        "    log_probs = self.nn_model(x_gpu).cpu().detach().numpy()\n",
        "    # return whole distribution over the words\n",
        "    return log_probs   \n",
        "\n",
        "  def generate_text(self, text_length):\n",
        "    \"\"\"This method generates random text of length \n",
        "    \n",
        "    For example\n",
        "    >>> lm.generate_text(2)\n",
        "    hello world\n",
        "\n",
        "    \"\"\"\n",
        "    # we begin from <s><s>... n-1 times\n",
        "    text = ['<s>'] * (self.n-1)\n",
        "    for j in range(text_length):\n",
        "      # evaluate probabilities of each word for current context\n",
        "      probs = self.predict_distribution(context=text[j:])[0]\n",
        "      probs = np.asarray(probs).astype('float64')\n",
        "      probs = probs / np.sum(probs)\n",
        "      # generate word index accordingly to distribution\n",
        "      rv = np.random.multinomial(1, probs, 1)\n",
        "      idx = rv.argmax()\n",
        "      # add word to text\n",
        "      text.append(self.vocab[idx])\n",
        "    # postprocess generated text\n",
        "    str_text = ' '. join(text)\n",
        "    str_text = str_text.replace('<s> ', \"\")\n",
        "    str_text = str_text.replace(' </s>', \"\")\n",
        "    return str_text\n",
        "  \n",
        "  @staticmethod\n",
        "  def construct_samples(text, n, word_to_id):\n",
        "      X, y = [], []\n",
        "      number_of_samples = len(text) - n + 1\n",
        "      for i in range(number_of_samples):\n",
        "        # if we get unknown word we encode it with number for 'unk' -> word_to_id['unk']\n",
        "        X.append(np.array([word_to_id.get(word, word_to_id['unk']) for word in text[i:i+n-1]]))\n",
        "        y.append(np.array(word_to_id.get(text[i+n-1], word_to_id['unk'])))\n",
        "      # convert to numpy for torch\n",
        "      X = np.array(X)\n",
        "      y = np.array(y)\n",
        "      return X, y\n",
        "\n",
        "  def update(self, sequence_of_tokens, val_sequence):\n",
        "    \"\"\"This method learns probabiities based on given sequence of tokents\n",
        "    \n",
        "    sequence_of_tokens -- iterable of tokens\n",
        "\n",
        "    For example\n",
        "    >>> lm.update(['hello', 'world'])\n",
        "    \"\"\"\n",
        "    print('Preprocessing stage')\n",
        "    # preprocess input sequences\n",
        "    normalized_sequence_train = self.preprocessor.normalize(sequence_of_tokens)\n",
        "    normalized_sequence_val = self.preprocessor.normalize(val_sequence)\n",
        "    # merge into one list of words\n",
        "    words_list_train, words_list_val = [], []\n",
        "    for sent in normalized_sequence_train:\n",
        "      words_list_train += sent \n",
        "\n",
        "    for sent in normalized_sequence_val:\n",
        "      words_list_val += sent\n",
        "    # count frequency of words in train\n",
        "    count_dict = Counter(words_list_train)\n",
        "    # find rare words\n",
        "    rare_words = []\n",
        "    for key, value in count_dict.items():\n",
        "      if value <= 2:\n",
        "        rare_words.append(key)\n",
        "    # substitute rare words with 'unk'\n",
        "    words_list_train = [x if x not in rare_words else 'unk' for x in words_list_train]\n",
        "    self.vocab = list(set(words_list_train))\n",
        "    self.vocab.append('unk')\n",
        "    self.voc_size = len(self.vocab)\n",
        "    \n",
        "\n",
        "    # create word - index and index - word mappings\n",
        "    self.word_to_id = {word:i for i, word in enumerate(self.vocab)}\n",
        "    self.id_to_word = {i:word for word, i in self.word_to_id.items()}\n",
        "\n",
        "    # create samples for training and validation\n",
        "    X_train, y_train = NeuralModel.construct_samples(words_list_train, self.n, self.word_to_id)\n",
        "    X_val, y_val = NeuralModel.construct_samples(words_list_val, self.n, self.word_to_id)\n",
        "\n",
        "    # prepare network\n",
        "    print('Start to train network')\n",
        "    # class for flatting  representation after embedding layer\n",
        "    class Flattener(nn.Module):\n",
        "      def forward(self, x):\n",
        "        batch_size, *_ = x.shape\n",
        "        return x.view(batch_size, -1)\n",
        "\n",
        "    # create model\n",
        "    self.nn_model = nn.Sequential(\n",
        "              nn.Embedding(self.voc_size, 10),\n",
        "              Flattener(),\n",
        "              nn.Linear((self.n-1)*10, 100),\n",
        "              nn.ReLU(inplace=True),\n",
        "              nn.Linear(100, self.voc_size),\n",
        "              nn.LogSoftmax(dim=1)\n",
        "          )\n",
        "    # use cuda\n",
        "    self.nn_model.type(torch.cuda.FloatTensor)\n",
        "    # define loss - cross - entropy without logarithm, because LogSoftmax is used\n",
        "    loss_function = nn.NLLLoss().type(torch.cuda.FloatTensor)\n",
        "    # define optimizer\n",
        "    optimizer = optim.SGD(self.nn_model.parameters(), lr=1e-2, weight_decay=1e-3)\n",
        "    # train network\n",
        "    self.__train_network(optimizer, loss_function, X_train, y_train, X_val, y_val)\n",
        "  \n",
        "  def __train_network(self, optimizer, loss_function, X_train, y_train, X_val, y_val):\n",
        "    train_subset_size = 10000\n",
        "    val_subset_size = 2000\n",
        "    batch_size = 1000\n",
        "    train_losses, val_losses = [], []\n",
        "    # create scheduler for lr annealing\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.2)\n",
        "\n",
        "    for epoch in range(self.n_epochs):\n",
        "      total_train_loss, total_val_loss = 0, 0\n",
        "      # enter train mode\n",
        "      self.nn_model.train()\n",
        "      # create random subset of train data\n",
        "      indices = list(range(X_train.shape[0]))\n",
        "      chosen_indices = np.random.choice(indices,train_subset_size)\n",
        "      X,y = X_train[chosen_indices,:], y_train[chosen_indices]\n",
        "      # train by batches\n",
        "      for i in range(train_subset_size // batch_size):\n",
        "        # create batch and send to GPU\n",
        "        context_idxs = torch.tensor(X[i:(i+1)*batch_size, :], dtype=torch.long)\n",
        "        y_gpu = torch.tensor(y[i:(i+1)*batch_size], dtype=torch.long).to(device)\n",
        "        x_gpu = context_idxs.to(device)\n",
        "        # delete previously accumulated gradients\n",
        "        self.nn_model.zero_grad()\n",
        "        # calc log probs\n",
        "        log_probs = self.nn_model(x_gpu)\n",
        "        # calc loss\n",
        "        loss = loss_function(log_probs, y_gpu)\n",
        "        # perform backward pass and update weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # calculate total loss\n",
        "        total_train_loss += loss.item() \n",
        "        \n",
        "      # enter evaluation mode\n",
        "      self.nn_model.eval()\n",
        "      # create random subset of train data\n",
        "      indices = list(range(X_val.shape[0]))\n",
        "      chosen_indices = np.random.choice(indices,val_subset_size)\n",
        "      X,y = X_val[chosen_indices,:], y_val[chosen_indices]\n",
        "\n",
        "      for i in range(val_subset_size // batch_size):\n",
        "        # create batch and send to GPU\n",
        "        context_idxs = torch.tensor(X[i:(i+1)*batch_size, :], dtype=torch.long)\n",
        "        y_gpu = torch.tensor(y[i:(i+1)*batch_size], dtype=torch.long).to(device)\n",
        "        x_gpu = context_idxs.to(device)\n",
        "        # calc log probs\n",
        "        log_probs = self.nn_model(x_gpu)\n",
        "        loss = loss_function(log_probs, y_gpu)\n",
        "        # calculate total loss\n",
        "        total_val_loss += loss.item() \n",
        "\n",
        "      print('Average train loss {}, average validation loss {}'.format(total_train_loss/train_subset_size, total_val_loss/val_subset_size))\n",
        "      train_losses.append(total_train_loss/train_subset_size)\n",
        "      val_losses.append(total_val_loss/val_subset_size)\n",
        "      scheduler.step()\n",
        "\n",
        "  def perplexity(self, sequence_of_tokens):\n",
        "    \"\"\"This method returns perplexity for a given sequence of tokens\n",
        "    \n",
        "    sequence_of_tokens -- iterable of tokens\n",
        "    \"\"\"\n",
        "    # preprocess input sequence\n",
        "    normalized_sequence = self.preprocessor.normalize(sequence_of_tokens)\n",
        "\n",
        "    # merge into one list of words\n",
        "    words_list = []\n",
        "    for sent in normalized_sequence:\n",
        "      words_list += sent \n",
        "\n",
        "    log_prob = 0\n",
        "    for i in range(self.n-1,len(words_list)):\n",
        "      log_prob += self.log_prob(word=words_list[i],\n",
        "                                   context=words_list[i-(self.n-1):i])\n",
        "    return np.exp(-log_prob/len(words_list))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MYqU16Ym_aO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_loader = TextLoader()\n",
        "full_text = text_loader.load_text_by_sentences(\"shakespeare_input.txt\")\n",
        "train_text, val_text, test_text = split(full_text, 0.5, 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6Psr8nRBaSs",
        "colab_type": "code",
        "outputId": "d4226d55-d0af-4ddd-bce7-b5bd690c46a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        }
      },
      "source": [
        "model = NeuralModel(n=5, n_epochs=50)\n",
        "model.update(train_text, val_text)\n",
        "print(model.perplexity(test_text))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing stage\n",
            "Start to train network\n",
            "Average train loss 0.008872530269622803, average validation loss 0.008822808742523194\n",
            "Average train loss 0.008779420757293701, average validation loss 0.00871520757675171\n",
            "Average train loss 0.008677464866638184, average validation loss 0.008610262393951415\n",
            "Average train loss 0.008549242115020752, average validation loss 0.008462195396423339\n",
            "Average train loss 0.008439209938049317, average validation loss 0.008347483158111572\n",
            "Average train loss 0.008312021923065186, average validation loss 0.008188549041748048\n",
            "Average train loss 0.008122458839416504, average validation loss 0.008070982456207276\n",
            "Average train loss 0.008002105808258057, average validation loss 0.00790249466896057\n",
            "Average train loss 0.007868156719207764, average validation loss 0.007753184080123901\n",
            "Average train loss 0.007770943832397461, average validation loss 0.007872731208801269\n",
            "Average train loss 0.007729489612579346, average validation loss 0.0077580418586730955\n",
            "Average train loss 0.007746487951278687, average validation loss 0.007708051919937134\n",
            "Average train loss 0.00763005223274231, average validation loss 0.00765809965133667\n",
            "Average train loss 0.007599412584304809, average validation loss 0.007650789260864258\n",
            "Average train loss 0.007631178236007691, average validation loss 0.007534067153930664\n",
            "Average train loss 0.007540397500991822, average validation loss 0.007449217557907104\n",
            "Average train loss 0.007535342216491699, average validation loss 0.007464207649230957\n",
            "Average train loss 0.007477112817764282, average validation loss 0.007399836301803589\n",
            "Average train loss 0.007519315338134765, average validation loss 0.00748617148399353\n",
            "Average train loss 0.0075031018257141115, average validation loss 0.0074673562049865725\n",
            "Average train loss 0.007485768032073975, average validation loss 0.00739473819732666\n",
            "Average train loss 0.007535166215896606, average validation loss 0.0074709982872009275\n",
            "Average train loss 0.0074462779521942135, average validation loss 0.007455598592758178\n",
            "Average train loss 0.007380301284790039, average validation loss 0.007472126007080078\n",
            "Average train loss 0.007370827198028565, average validation loss 0.007325859308242798\n",
            "Average train loss 0.00741246452331543, average validation loss 0.007505101442337036\n",
            "Average train loss 0.007443952035903931, average validation loss 0.007366640090942383\n",
            "Average train loss 0.007485830354690552, average validation loss 0.0074196231365203855\n",
            "Average train loss 0.007406241226196289, average validation loss 0.007309617280960083\n",
            "Average train loss 0.007441415071487427, average validation loss 0.0073899636268615725\n",
            "Average train loss 0.00742200984954834, average validation loss 0.007366542339324951\n",
            "Average train loss 0.007485571050643921, average validation loss 0.007534370422363281\n",
            "Average train loss 0.007430520105361938, average validation loss 0.007368491888046265\n",
            "Average train loss 0.007501843643188477, average validation loss 0.007398330926895141\n",
            "Average train loss 0.007426295804977417, average validation loss 0.0074583210945129395\n",
            "Average train loss 0.0074864102363586425, average validation loss 0.007417624950408936\n",
            "Average train loss 0.007473774099349975, average validation loss 0.00734683346748352\n",
            "Average train loss 0.007408013772964478, average validation loss 0.007476192235946655\n",
            "Average train loss 0.0074320624351501465, average validation loss 0.007440945148468017\n",
            "Average train loss 0.007454302167892456, average validation loss 0.007373558282852173\n",
            "Average train loss 0.007438456487655639, average validation loss 0.0075400185585021975\n",
            "Average train loss 0.007381002378463745, average validation loss 0.007244513988494873\n",
            "Average train loss 0.0074441154003143314, average validation loss 0.007397575616836548\n",
            "Average train loss 0.007463560771942139, average validation loss 0.00728506350517273\n",
            "Average train loss 0.007448984670639038, average validation loss 0.007411027908325195\n",
            "Average train loss 0.0074024475574493405, average validation loss 0.007299759387969971\n",
            "Average train loss 0.007478664112091064, average validation loss 0.007319095134735107\n",
            "Average train loss 0.007452473640441894, average validation loss 0.0074670474529266355\n",
            "Average train loss 0.007403122425079346, average validation loss 0.0073963794708251955\n",
            "Average train loss 0.007432350397109985, average validation loss 0.007492067813873291\n",
            "1788.7840630939183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D62A3VJhszMq",
        "colab_type": "code",
        "outputId": "997b0350-83fa-406f-cbc4-9cc9b4ff8fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        }
      },
      "source": [
        "model = NeuralModel(n=3, n_epochs=50)\n",
        "model.update(train_text, val_text)\n",
        "print(model.perplexity(test_text))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing stage\n",
            "Start to train network\n",
            "Average train loss 0.00893566017150879, average validation loss 0.008937527656555175\n",
            "Average train loss 0.00890982484817505, average validation loss 0.00890673542022705\n",
            "Average train loss 0.008900097465515136, average validation loss 0.008898905277252197\n",
            "Average train loss 0.008865694999694824, average validation loss 0.008864830017089844\n",
            "Average train loss 0.00885314702987671, average validation loss 0.008852096080780029\n",
            "Average train loss 0.008834748840332031, average validation loss 0.00882112693786621\n",
            "Average train loss 0.008803851699829102, average validation loss 0.008808919906616211\n",
            "Average train loss 0.008784897804260254, average validation loss 0.008763750076293946\n",
            "Average train loss 0.00877230453491211, average validation loss 0.008773267269134521\n",
            "Average train loss 0.008772433280944824, average validation loss 0.0087579345703125\n",
            "Average train loss 0.008766335487365723, average validation loss 0.008757902145385741\n",
            "Average train loss 0.008766752529144287, average validation loss 0.008769524097442628\n",
            "Average train loss 0.008756381034851074, average validation loss 0.00876401662826538\n",
            "Average train loss 0.008744717216491699, average validation loss 0.008756598949432374\n",
            "Average train loss 0.0087426438331604, average validation loss 0.00873526382446289\n",
            "Average train loss 0.00874012508392334, average validation loss 0.008748788833618164\n",
            "Average train loss 0.00874853858947754, average validation loss 0.008752561569213868\n",
            "Average train loss 0.008738075923919678, average validation loss 0.008732364177703858\n",
            "Average train loss 0.008736276626586914, average validation loss 0.008732962608337402\n",
            "Average train loss 0.00874062385559082, average validation loss 0.008727965831756591\n",
            "Average train loss 0.008741841506958007, average validation loss 0.008746084690093994\n",
            "Average train loss 0.008744362831115723, average validation loss 0.008745038986206054\n",
            "Average train loss 0.0087230712890625, average validation loss 0.00874493169784546\n",
            "Average train loss 0.008726485919952393, average validation loss 0.00873777198791504\n",
            "Average train loss 0.008736259746551513, average validation loss 0.008751677989959716\n",
            "Average train loss 0.008735498237609864, average validation loss 0.008735079288482666\n",
            "Average train loss 0.008726769256591797, average validation loss 0.00872503137588501\n",
            "Average train loss 0.00873617696762085, average validation loss 0.008729865550994873\n",
            "Average train loss 0.008731600666046143, average validation loss 0.00874716329574585\n",
            "Average train loss 0.008720236968994141, average validation loss 0.00871662998199463\n",
            "Average train loss 0.008728707504272461, average validation loss 0.008713163852691651\n",
            "Average train loss 0.008720122909545899, average validation loss 0.008716878414154052\n",
            "Average train loss 0.008729250144958496, average validation loss 0.008732443809509277\n",
            "Average train loss 0.008737617588043213, average validation loss 0.00874159049987793\n",
            "Average train loss 0.00872730941772461, average validation loss 0.008738875389099121\n",
            "Average train loss 0.0087370512008667, average validation loss 0.008729381084442139\n",
            "Average train loss 0.00872569637298584, average validation loss 0.008723675727844238\n",
            "Average train loss 0.008733764362335205, average validation loss 0.008736887454986571\n",
            "Average train loss 0.008723772621154785, average validation loss 0.008734546661376952\n",
            "Average train loss 0.008723691082000733, average validation loss 0.008725813865661621\n",
            "Average train loss 0.008735669803619384, average validation loss 0.008734295845031739\n",
            "Average train loss 0.008731999397277833, average validation loss 0.008730873584747315\n",
            "Average train loss 0.008720787715911865, average validation loss 0.008727928161621094\n",
            "Average train loss 0.0087285945892334, average validation loss 0.008736945152282715\n",
            "Average train loss 0.00872089490890503, average validation loss 0.008722405433654785\n",
            "Average train loss 0.008724541759490967, average validation loss 0.008717727184295655\n",
            "Average train loss 0.00873337001800537, average validation loss 0.008742598533630372\n",
            "Average train loss 0.008729993152618408, average validation loss 0.008717926979064941\n",
            "Average train loss 0.008724986934661866, average validation loss 0.008728600025177002\n",
            "Average train loss 0.00872848768234253, average validation loss 0.008730291843414307\n",
            "6247.387389697919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTNuCVSbJ3lc",
        "colab_type": "code",
        "outputId": "644d588b-d6f5-47f5-e6d5-c58b1df7c11a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(model.generate_text(100))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "celebrate flow sails mouth graver festival dares wonderful gathered father dagger silken rule false curious valentine breeding strikes resolute sans hard-hearted whining chosen knots extremest moment lief downright northumberland taught both devils steward flock break whore necks cordial vows everlastingly apollo wheat received reconciled followers passion labours tearing those fatal hoop slily expose lips decorum except yield drove notorious split offend bending dishes aimed party bonds treacherous maidenhead unadvised each way wander butchers price wisdoms admired cheerly remembers liberties cowards ulysses remainder longest saying maiden advancement exigent blowing daughter governor game crowns chosen lieu isabel begged flow helping rule offender\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdLoztJ2Pla6",
        "colab_type": "text"
      },
      "source": [
        "Neural network can obtain better results, than n-gram model. But results are unstable."
      ]
    }
  ]
}